{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1H1pfmz1ddPG"
   },
   "source": [
    "# MNIST \n",
    "\n",
    "This example is an implementation of federated learning using Substra's Distributed Learning Contributivity.\n",
    "\n",
    "This is based on both [existing resources on MNIST](https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e) and [precedent implementation of this dataset for the standalone application](https://github.com/SubstraFoundation/distributed-learning-contributivity/blob/master/datasets/dataset_mnist.py).\n",
    "\n",
    "This notebook will be focused on importing manually the dataset, do a bit of preprocessing and build our objects to run a collaborative round.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "In order to run this example, you'll need to:\n",
    "\n",
    "* use python 3.7 +\n",
    "* install requierements from the requirements.txt file\n",
    "* install this package https://test.pypi.org/project/pkg-test-distributed-learning-contributivity/0.0.7/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NXZm5jUGdpt6",
    "outputId": "ef7e58cb-7acc-45a9-c068-00ba8fd8d2b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exâ€šcutable ou un fichier de commandes.\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from subtest==0.0.0.10) (from versions: none)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Collecting subtest==0.0.0.10\n",
      "  Downloading https://test-files.pythonhosted.org/packages/b8/5a/2540de2b5d53a93ddb6513c0aed65aef8e964b7d9d951011055bcc55fa61/subtest-0.0.0.10-py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: librosa==0.8.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.8.0)\n",
      "Requirement already satisfied: scikit-learn==0.22.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.22.1)\n",
      "Requirement already satisfied: loguru==0.4.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR: No matching distribution found for tensorflow==2.2.0 (from subtest==0.0.0.10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ruamel.yaml==0.16.10 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.16.10)\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/SubstraFoundation/distributed-learning-contributivity/Moving-functions/requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "!pip install -i https://test.pypi.org/simple/ subtest==0.0.0.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "Dv0WF-jEddPL",
    "outputId": "ad7b4fdc-68c4-4f38-88a5-7e018cc04273"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "r8ZI1KswddPN",
    "outputId": "8d8907cf-4b08-47f6-a6f2-b96df350658f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\arthu\\Anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Object and methodes needed in order to run a collaborative round\n",
    "from subtest.datasets.dataset import Dataset\n",
    "from subtest.scenario import Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrYrGPY1ddPP"
   },
   "source": [
    "# Create a custom scenario handling mandatory parameters\n",
    "\n",
    "These parameters describe how many partners will be created and how much proportion they will have in the dataset.\n",
    "\n",
    "We can use more advanced sample split options in order to fine tune the data distribution between partners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qD3t3vT3ddPS"
   },
   "source": [
    "# Set values for scenario parameters\n",
    "\n",
    "## Mandatory parameters\n",
    "partner_count and amounts_per_partner describe how many partners will be created and how much proportion they will have in the dataset. Here we choose 4 partners, with respectively 20 %, 50% and 30% of the dataset. \n",
    "\n",
    "We can use more advanced sample split options in order to fine tune the data distribution between partners.\n",
    "\n",
    "## Optionnal parameters\n",
    "\n",
    "We want our training to go for 10 epochs and 3 minibatches per epoch.\n",
    "\n",
    "Moreover, there is 4 datasets which are pre-implemented in subtest, but for the example we will create our own dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2s7grIQmddPX"
   },
   "source": [
    "# Create Data Set\n",
    "\n",
    "For this experiment we use the well known MNIST dataset.\n",
    "\n",
    "This example is also available using the standalone app specifying in the config file : dataset_name: - 'mnist', or by passing the parameter dataset_name = 'mnist' to Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMKAQispddPX"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],  28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3CfWhs8ddPZ"
   },
   "source": [
    "# Create Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtjWVBXAddPa"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset_labels(y):\n",
    "    y = np_utils.to_categorical(y, 10)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-Ygu456ddPe"
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxHt1CUAddPe"
   },
   "outputs": [],
   "source": [
    "def generate_new_model_for_dataset():\n",
    "    model = Sequential()\n",
    "    # add Convolutional layers\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))    \n",
    "    model.add(Flatten())\n",
    "    # Densely connected layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # compile with adam optimizer & categorical_crossentropy loss function\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1e-21brZddPg"
   },
   "source": [
    "# Generate dataset\n",
    "\n",
    "Note that the scenario needs a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    \"my_dataset\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    preprocess_dataset_labels,\n",
    "    generate_new_model_for_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'train_val_split_local'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-e0cfd64da5f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_val_split_local\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'train_val_split_local'"
     ]
    }
   ],
   "source": [
    "dataset.train_val_split_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CztnBVbPddPU"
   },
   "source": [
    "#### Every other parameter will be set to its default value\n",
    "\n",
    "We might consider :\n",
    "\n",
    "- Datas will be split randomly between partner\n",
    "- The learning approach is 'fedavg' for federated averaging \n",
    "- Weights will be averaged uniformly, different weights can be applied for each partner\n",
    "\n",
    "The learning approaches are built-in paramater that can be set easily. There are currently 4 differents approaches.\n",
    "\n",
    "\n",
    "#### More details at : https://github.com/SubstraFoundation/distributed-learning-contributivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaTh2os1ddPU"
   },
   "source": [
    "# Define scenario\n",
    "\n",
    "We specify our experiment path used to output graphs and results.\n",
    "\n",
    "We can now create the scenario that will handle every parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "7ZU1TCA2ddPV",
    "outputId": "0180cd4d-b108-44db-e24b-d366baa6297d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 10:28:08.235 | DEBUG    | subtest.scenario:__init__:101 - Computation use the full dataset for scenario #1\n",
      "2020-09-16 10:28:08.579 | INFO     | subtest.scenario:__init__:262 - ### Description of data scenario configured:\n",
      "2020-09-16 10:28:08.580 | INFO     | subtest.scenario:__init__:263 -    Number of partners defined: 3\n",
      "2020-09-16 10:28:08.581 | INFO     | subtest.scenario:__init__:264 -    Data distribution scenario chosen: random\n",
      "2020-09-16 10:28:08.581 | INFO     | subtest.scenario:__init__:265 -    Multi-partner learning approach: fedavg\n",
      "2020-09-16 10:28:08.582 | INFO     | subtest.scenario:__init__:266 -    Weighting option: uniform\n",
      "2020-09-16 10:28:08.583 | INFO     | subtest.scenario:__init__:267 -    Iterations parameters: 10 epochs > 3 mini-batches > 8 gradient updates per pass\n",
      "2020-09-16 10:28:08.584 | INFO     | subtest.scenario:__init__:273 - ### Data loaded: my_dataset\n",
      "2020-09-16 10:28:08.585 | INFO     | subtest.scenario:__init__:274 -    54000 train data with 54000 labels\n",
      "2020-09-16 10:28:08.586 | INFO     | subtest.scenario:__init__:275 -    6000 val data with 6000 labels\n",
      "2020-09-16 10:28:08.587 | INFO     | subtest.scenario:__init__:276 -    10000 test data with 10000 labels\n"
     ]
    }
   ],
   "source": [
    "current_scenario = Scenario(partners_count = 3,\n",
    "                            amounts_per_partner = [0.2, 0.5, 0.3],\n",
    "                            epoch_count = 10,\n",
    "                            minibatch_count = 3,\n",
    "                            dataset = dataset\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVLFZNorddPk"
   },
   "source": [
    "# Run scenario\n",
    "\n",
    "The actual training phase of our federated learning example ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5HA_xIvsddPl",
    "outputId": "f0d9b4eb-2385-48e2-87c5-a058053d0fad"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'train_val_split_local'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0b97c1569165>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcurrent_scenario\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Documents\\Substra\\repos\\mplc\\distributed-learning-contributivity\\subtest\\scenario.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;31m# ... train data, early stopping validation data, test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_split_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'basic'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_split_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'advanced'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_data_advanced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Substra\\repos\\mplc\\distributed-learning-contributivity\\subtest\\scenario.py\u001b[0m in \u001b[0;36msplit_data\u001b[1;34m(self, is_logging_enabled)\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[1;31m# Create local validation and test datasets from the partner train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_test_split_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_val_split_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m             \u001b[1;31m# Update other attributes from partner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'train_val_split_local'"
     ]
    }
   ],
   "source": [
    "current_scenario.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeWcWlC5ddPn"
   },
   "source": [
    "# Results\n",
    "\n",
    "We can see every parameter used pre and post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "X67YaGm3ddPn",
    "outputId": "5a8ba01f-ef5f-426f-ec7a-5c07471f6245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aggregation_weighting', 'dataset_fraction_per_partner', 'dataset_name',\n",
      "       'epoch_count', 'final_relative_nb_samples',\n",
      "       'gradient_updates_per_pass_count', 'is_early_stopping',\n",
      "       'learning_computation_time_sec', 'minibatch_count',\n",
      "       'mpl_nb_epochs_done', 'mpl_test_score',\n",
      "       'multi_partner_learning_approach', 'nb_samples_used', 'partners_count',\n",
      "       'samples_split_description', 'scenario_name', 'short_scenario_name',\n",
      "       'test_data_samples_count', 'train_data_samples_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_results = current_scenario.to_dataframe()\n",
    "print(df_results.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XW_EH1fxddPp"
   },
   "source": [
    "#### Our score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "hUBKGXfXddPp",
    "outputId": "c8d03c0d-1501-4d3b-96c5-e1e29fd4e49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach used : fedavg\n",
      "Model accuracy : 0.9811000227928162\n",
      "0    uniform\n",
      "Name: aggregation_weighting, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Approach used :\", df_results.multi_partner_learning_approach[0])\n",
    "print(\"Model accuracy :\", df_results.mpl_test_score[0])\n",
    "print(df_results.aggregation_weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99jaKK2LddPr"
   },
   "source": [
    "## Extract model \n",
    "\n",
    "We can extract our model and save it for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wIdoS_bddPr"
   },
   "outputs": [],
   "source": [
    "model = current_scenario.mpl.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "M4azOyxqddPu",
    "outputId": "3a4a8904-f156-4e96-b29b-1ff5c7faa60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 65us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05563341381018981, 0.9811000227928162]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, preprocess_dataset_labels(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VheE9NyxddPw"
   },
   "source": [
    "# That's it !\n",
    "\n",
    "Now you can explore our other tutorials for a better snapshot of what can be done with our library!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "1 _INTRO_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}